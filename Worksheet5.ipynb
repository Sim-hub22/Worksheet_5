{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "3.1 Implementation from Scratch Step - by - Step Guide:"
      ],
      "metadata": {
        "id": "hE-ZxV3AZhxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "• To - Do - 1:\n",
        "1. Read and Observe the Dataset.\n",
        "2. Print top(5) and bottom(5) of the dataset {Hint: pd.head and pd.tail}.\n",
        "3. Print the Information of Datasets. {Hint: pd.info}.\n",
        "4. Gather the Descriptive info about the Dataset. {Hint: pd.describe}\n",
        "5. Split your data into Feature (X) and Label (Y)."
      ],
      "metadata": {
        "id": "qmjV-zLJZkt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/student.csv\")\n",
        "\n",
        "# View the first 5 rows\n",
        "print(df.head())\n",
        "\n",
        "# View the last 5 rows\n",
        "print(df.tail())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQjZaR2MZm6e",
        "outputId": "b9a1178a-8a07-4527-ec7f-39b39f3dc342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Math  Reading  Writing\n",
            "0    48       68       63\n",
            "1    62       81       72\n",
            "2    79       80       78\n",
            "3    76       83       79\n",
            "4    59       64       62\n",
            "     Math  Reading  Writing\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display basic information about the dataset\n",
        "print(df.info())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD_udR03aFBs",
        "outputId": "1e55b8d5-0f23-44b7-f004-235dff3b36c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Math     1000 non-null   int64\n",
            " 1   Reading  1000 non-null   int64\n",
            " 2   Writing  1000 non-null   int64\n",
            "dtypes: int64(3)\n",
            "memory usage: 23.6 KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get descriptive statistics for the dataset\n",
        "print(df.describe())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYeJ9k1AaINK",
        "outputId": "7c49da14-3644-426a-b807-37155df1f66d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              Math      Reading      Writing\n",
            "count  1000.000000  1000.000000  1000.000000\n",
            "mean     67.290000    69.872000    68.616000\n",
            "std      15.085008    14.657027    15.241287\n",
            "min      13.000000    19.000000    14.000000\n",
            "25%      58.000000    60.750000    58.000000\n",
            "50%      68.000000    70.000000    69.500000\n",
            "75%      78.000000    81.000000    79.000000\n",
            "max     100.000000   100.000000   100.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the label (Y) and features (X)\n",
        "# Assuming 'target' is the column name for the label\n",
        "X = df.drop(columns=['Writing'])  # Drop the target column for features\n",
        "Y = df['Writing']  # Define the target variable\n"
      ],
      "metadata": {
        "id": "k64dnlS1aNnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "• To - Do - 2:\n",
        "1. To make the task easier - let’s assume there is no bias or intercept.\n",
        "2. Create the following matrices:\n",
        "\n",
        "Y = W^TX"
      ],
      "metadata": {
        "id": "zEdKW4-LaRkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Let's assume X is the feature matrix and Y is the target variable from the previous step.\n",
        "# X.shape = (d, n) where d = number of features, n = number of samples\n",
        "\n",
        "# Example for a random dataset (for the sake of illustration)\n",
        "# Let's assume we have d=3 features and n=5 samples\n",
        "d, n = 3, 5\n",
        "\n",
        "# Random feature matrix X (d x n)\n",
        "X = np.random.randn(d, n)\n",
        "\n",
        "# Random weights vector W (d x 1)\n",
        "W = np.random.randn(d, 1)\n",
        "\n",
        "# The target Y (n x 1) is the result of W^T * X\n",
        "Y = np.dot(W.T, X)\n",
        "\n",
        "# Print the matrices to check them\n",
        "print(\"X (Feature matrix):\\n\", X)\n",
        "print(\"\\nW (Weight vector):\\n\", W)\n",
        "print(\"\\nY (Target vector):\\n\", Y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfSYcWAsaVIw",
        "outputId": "022ccef8-c8a0-4c57-e04d-ddf64543f8ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X (Feature matrix):\n",
            " [[ 0.54971536 -0.81577152 -0.55652048  1.51203496 -1.60939558]\n",
            " [-0.50290903 -0.26079956  1.63415034 -0.22107669 -1.68429796]\n",
            " [ 1.34640207 -1.38029469 -0.15277859 -0.62936342 -0.55831545]]\n",
            "\n",
            "W (Weight vector):\n",
            " [[-1.70831152]\n",
            " [-0.48441215]\n",
            " [ 1.3617449 ]]\n",
            "\n",
            "Y (Target vector):\n",
            " [[ 1.13798631 -0.3596829  -0.0489374  -3.33296693  2.80496017]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To - Do - 3:\n",
        "1. Split the dataset into training and test sets.\n",
        "2. You can use an 80-20 or 70-30 split, with 80% (or 70%) of the data used for training and the rest\n",
        "for testing."
      ],
      "metadata": {
        "id": "KmjV8yzsaaVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset into training and test sets (80% training, 20% test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X.T, Y.T, test_size=0.2, random_state=42)\n",
        "\n",
        "# X_train and Y_train will be used for training the model\n",
        "# X_test and Y_test will be used for testing the model\n",
        "\n",
        "# Print the shapes of the resulting datasets\n",
        "print(\"Training Features Shape: \", X_train.shape)\n",
        "print(\"Training Labels Shape: \", Y_train.shape)\n",
        "print(\"Test Features Shape: \", X_test.shape)\n",
        "print(\"Test Labels Shape: \", Y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_LS-YOtac6o",
        "outputId": "e2f3b8b9-c7d7-4292-eab9-7d212fdbbb8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Features Shape:  (4, 3)\n",
            "Training Labels Shape:  (4, 1)\n",
            "Test Features Shape:  (1, 3)\n",
            "Test Labels Shape:  (1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 70% training, 30% test\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X.T, Y.T, test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "_KF7BDPPahma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.2 Step -2- Build a Cost Function:"
      ],
      "metadata": {
        "id": "pWtNhLN1akjZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost function is the average of loss function measured across the data point. As the cost function for Regression\n",
        "problem we will be using Mean Square Error which is given by:\n"
      ],
      "metadata": {
        "id": "EuGB9EdHamOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cost_function(X, Y, W):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    X: Feature Matrix (d x n) where d is the number of features and n is the number of samples\n",
        "    Y: Target Matrix (1 x n) where n is the number of samples\n",
        "    W: Weight Matrix (d x 1)\n",
        "\n",
        "    Output:\n",
        "    cost: The accumulated Mean Squared Error\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute the predictions Y_pred = W^T * X\n",
        "    Y_pred = np.dot(W.T, X)\n",
        "\n",
        "    # Compute the Mean Squared Error cost\n",
        "    # Cost function formula: L(W) = (1 / 2n) * sum((Y_pred - Y)^2)\n",
        "    n = X.shape[1]  # Number of samples\n",
        "    cost = (1 / (2 * n)) * np.sum((Y_pred - Y) ** 2)\n",
        "\n",
        "    return cost"
      ],
      "metadata": {
        "id": "TmANWQ5XaxtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example Usage:"
      ],
      "metadata": {
        "id": "fGMmYf29a0xF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example feature matrix X (3 features, 5 samples)\n",
        "X = np.array([[1, 2, 3],\n",
        "              [4, 5, 6],\n",
        "              [7, 8, 9]])\n",
        "\n",
        "# Example target vector Y (1 x 3)\n",
        "Y = np.array([1, 2, 3])\n",
        "\n",
        "# Example weight vector W (3 x 1)\n",
        "W = np.array([[0.1],\n",
        "              [0.2],\n",
        "              [0.3]])\n",
        "\n",
        "# Calculate the cost\n",
        "cost = cost_function(X, Y, W)\n",
        "print(\"Cost:\", cost)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HUDNoJha2Of",
        "outputId": "4c0ef515-f5cf-40f1-8333-9db0e8f2edf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost: 1.333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Designing a Test Case for Cost Function:"
      ],
      "metadata": {
        "id": "05ZyED8IcQeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the cost function as previously provided\n",
        "def cost_function(X, Y, W):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    X: Feature Matrix (d x n) where d is the number of features and n is the number of samples\n",
        "    Y: Target Matrix (1 x n) where n is the number of samples\n",
        "    W: Weight Matrix (d x 1)\n",
        "\n",
        "    Output:\n",
        "    cost: The accumulated Mean Squared Error\n",
        "    \"\"\"\n",
        "\n",
        "    # Reshape W to ensure correct dimensionality for matrix multiplication\n",
        "    W = W.reshape(-1, 1)  # Convert W to a column vector of shape (d, 1)\n",
        "\n",
        "    # Compute the predictions Y_pred = X * W\n",
        "    Y_pred = np.dot(X, W)  # Corrected: Compute dot product of X and W, no transpose needed\n",
        "\n",
        "    # Compute the Mean Squared Error cost\n",
        "    n = X.shape[0]  # Number of samples (this is correct as we're using X with shape (d, n))\n",
        "    cost = (1 / (2 * n)) * np.sum((Y_pred - Y.reshape(-1, 1)) ** 2)  # Ensure Y is reshaped as a column vector\n",
        "\n",
        "    return cost\n",
        "\n",
        "# Test case\n",
        "X_test = np.array([[1, 2], [3, 4], [5, 6]])  # 3 samples, 2 features\n",
        "Y_test = np.array([3, 7, 11])  # 3 samples, the target values\n",
        "W_test = np.array([1, 1])  # Weight vector (2 features)\n",
        "\n",
        "# Compute the cost using the cost_function\n",
        "cost = cost_function(X_test, Y_test, W_test)\n",
        "\n",
        "# Verify if the output is as expected (0)\n",
        "if cost == 0:\n",
        "    print(\"Proceed Further\")\n",
        "else:\n",
        "    print(\"Something went wrong: Reimplement the cost function\")\n",
        "\n",
        "# Output the cost function result\n",
        "print(\"Cost function output:\", cost)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddYc9PCvc3IJ",
        "outputId": "49b999b9-7715-42f6-eff7-33997ee75a75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proceed Further\n",
            "Cost function output: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the cost function as previously provided\n",
        "def cost_function(X, Y, W):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    X: Feature Matrix (d x n) where d is the number of features and n is the number of samples\n",
        "    Y: Target Matrix (1 x n) where n is the number of samples\n",
        "    W: Weight Matrix (d x 1)\n",
        "\n",
        "    Output:\n",
        "    cost: The accumulated Mean Squared Error\n",
        "    \"\"\"\n",
        "\n",
        "    # Reshape W to ensure correct dimensionality for matrix multiplication\n",
        "    W = W.reshape(-1, 1)  # Convert W to a column vector of shape (2, 1)\n",
        "\n",
        "    # Compute the predictions Y_pred = X * W\n",
        "    # Changed from X.T to X to fix the dimension mismatch\n",
        "    Y_pred = np.dot(X, W)  # Transpose X to match dimensions with W\n",
        "\n",
        "    # Compute the Mean Squared Error cost\n",
        "    # Changed from X.shape[1] to X.shape[0]\n",
        "    # because the number of samples is now the first dimension\n",
        "    n = X.shape[0]  # Number of samples\n",
        "    cost = (1 / (2 * n)) * np.sum((Y_pred - Y) ** 2)\n",
        "\n",
        "    return cost\n",
        "\n",
        "# Test case\n",
        "X_test = np.array([[1, 2], [3, 4], [5, 6]])  # 2 features, 3 samples\n",
        "Y_test = np.array([3, 7, 11])  # 3 samples, the target values\n",
        "W_test = np.array([1, 1])  # Weight vector (2 features)\n",
        "\n",
        "# Compute the cost using the cost_function\n",
        "cost = cost_function(X_test, Y_test, W_test)\n",
        "\n",
        "# Verify if the output is as expected (0)\n",
        "if cost == 0:\n",
        "    print(\"Proceed Further\")\n",
        "else:\n",
        "    print(\"Something went wrong: Reimplement the cost function\")\n",
        "\n",
        "# Output the cost function result\n",
        "print(\"Cost function output:\", cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wbPS3unc-Yf",
        "outputId": "a9fb7ad5-e1e3-4509-e21c-535af531159a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Something went wrong: Reimplement the cost function\n",
            "Cost function output: 32.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.3 Step -3- Gradient Descent for Simple Linear Regression:"
      ],
      "metadata": {
        "id": "b5aOe8xzdDXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Gradient Descent function definition\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize the parameters of a linear regression model.\n",
        "    Parameters:\n",
        "    X (numpy.ndarray): Feature matrix (m x n).\n",
        "    Y (numpy.ndarray): Target vector (m x 1).\n",
        "    W (numpy.ndarray): Initial guess for parameters (n x 1).\n",
        "    alpha (float): Learning rate.\n",
        "    iterations (int): Number of iterations for gradient descent.\n",
        "    Returns:\n",
        "    tuple: A tuple containing the final optimized parameters (W_update) and the history of cost values.\n",
        "    \"\"\"\n",
        "    # Initialize cost history\n",
        "    cost_history = [0] * iterations\n",
        "    # Number of samples\n",
        "    m = len(Y)\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "        # Step 1: Hypothesis Values\n",
        "        Y_pred = np.dot(X, W)  # Linear prediction: Y_pred = X * W\n",
        "\n",
        "        # Step 2: Difference between Hypothesis and Actual Y\n",
        "        loss = Y_pred - Y\n",
        "\n",
        "        # Step 3: Gradient Calculation\n",
        "        dw = (1/m) * np.dot(X.T, loss)  # Gradient of the loss function\n",
        "\n",
        "        # Step 4: Updating Values of W using Gradient\n",
        "        W_update = W - alpha * dw  # Update weights\n",
        "\n",
        "        # Step 5: New Cost Value\n",
        "        cost = (1/(2*m)) * np.sum(loss ** 2)  # Mean Squared Error cost\n",
        "        cost_history[iteration] = cost  # Store the cost\n",
        "\n",
        "        # Update weights for next iteration\n",
        "        W = W_update\n",
        "\n",
        "    return W_update, cost_history\n",
        "\n",
        "# Generate random test data\n",
        "np.random.seed(0)  # For reproducibility\n",
        "X = np.random.rand(100, 3)  # 100 samples, 3 features\n",
        "Y = np.random.rand(100)  # 100 target values\n",
        "W = np.random.rand(3)  # Initial guess for parameters\n",
        "\n",
        "# Set hyperparameters\n",
        "alpha = 0.01  # Learning rate\n",
        "iterations = 1000  # Number of iterations\n",
        "\n",
        "# Test the gradient_descent function\n",
        "final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
        "\n",
        "# Print the final parameters and cost history\n",
        "print(\"Final Parameters:\", final_params)\n",
        "print(\"Cost History (last 10 values):\", cost_history[-10:])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VojybVwkdoDp",
        "outputId": "7ebec3e7-a757-4069-e6b2-974046c58911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Parameters: [0.20551667 0.54295081 0.10388027]\n",
            "Cost History (last 10 values): [0.05438688763901759, 0.054383665685533336, 0.0543804494134437, 0.054377238812615865, 0.05437403387293539, 0.054370834584306166, 0.05436764093665037, 0.054364452919908414, 0.05436127052403898, 0.05435809373901896]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.4 Step -4- Evaluate the Model:"
      ],
      "metadata": {
        "id": "R4pHlMKrdvGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Model Evaluation - RMSE\n",
        "def rmse(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    This function calculates the Root Mean Squared Error (RMSE).\n",
        "\n",
        "    Parameters:\n",
        "    Y (numpy.ndarray): Array of actual (target) dependent variables.\n",
        "    Y_pred (numpy.ndarray): Array of predicted dependent variables.\n",
        "\n",
        "    Returns:\n",
        "    float: The Root Mean Squared Error (RMSE).\n",
        "    \"\"\"\n",
        "    # Compute the squared differences between actual and predicted values\n",
        "    squared_diff = (Y - Y_pred) ** 2\n",
        "\n",
        "    # Compute the mean of the squared differences\n",
        "    mean_squared_diff = np.mean(squared_diff)\n",
        "\n",
        "    # Compute the square root of the mean squared differences\n",
        "    rmse_value = np.sqrt(mean_squared_diff)\n",
        "\n",
        "    return rmse_value\n",
        "\n",
        "# Example usage\n",
        "Y_test = np.array([3, 7, 11])  # Actual target values\n",
        "Y_pred = np.array([2.8, 6.9, 11.2])  # Predicted values\n",
        "\n",
        "# Compute RMSE\n",
        "error = rmse(Y_test, Y_pred)\n",
        "\n",
        "print(\"Root Mean Squared Error (RMSE):\", error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anx_uwlpdyad",
        "outputId": "f45961bb-2065-4096-abc4-c5979c1613ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error (RMSE): 0.17320508075688745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for R-Squared Error:"
      ],
      "metadata": {
        "id": "PfcwNstheLU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Model Evaluation - R2\n",
        "def r2(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    This function calculates the R Squared (R²) score.\n",
        "\n",
        "    Parameters:\n",
        "    Y (numpy.ndarray): Array of actual (target) dependent variables.\n",
        "    Y_pred (numpy.ndarray): Array of predicted dependent variables.\n",
        "\n",
        "    Returns:\n",
        "    float: The R Squared (R²) score.\n",
        "    \"\"\"\n",
        "    # Mean of the actual values\n",
        "    mean_y = np.mean(Y)\n",
        "\n",
        "    # Total Sum of Squares (SS_tot)\n",
        "    ss_tot = np.sum((Y - mean_y) ** 2)\n",
        "\n",
        "    # Residual Sum of Squares (SS_res)\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
        "\n",
        "    # R Squared score\n",
        "    r2 = 1 - (ss_res / ss_tot)\n",
        "\n",
        "    return r2\n",
        "\n",
        "# Example usage\n",
        "Y_test = np.array([3, 7, 11])  # Actual target values\n",
        "Y_pred = np.array([2.8, 6.9, 11.2])  # Predicted values\n",
        "\n",
        "# Compute R Squared\n",
        "r_squared = r2(Y_test, Y_pred)\n",
        "\n",
        "print(\"R Squared (R²):\", r_squared)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbZJ2RZseL4W",
        "outputId": "0d69856f-dafa-4bd5-e4dc-e814cdd6c5ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R Squared (R²): 0.9971875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.5 Step -5- Main Function to Integrate All Steps:"
      ],
      "metadata": {
        "id": "6fsxkjuWeZlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Gradient Descent function\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize the parameters of a linear regression model.\n",
        "    \"\"\"\n",
        "    m = len(Y)\n",
        "    cost_history = [0] * iterations\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "        Y_pred = np.dot(X, W)  # Linear prediction: Y_pred = X * W\n",
        "        loss = Y_pred - Y  # Difference between prediction and actual values\n",
        "        dw = (1/m) * np.dot(X.T, loss)  # Gradient of the loss function\n",
        "        W = W - alpha * dw  # Update weights\n",
        "\n",
        "        # Compute the cost (Mean Squared Error)\n",
        "        cost = (1/(2*m)) * np.sum(loss ** 2)\n",
        "        cost_history[iteration] = cost\n",
        "\n",
        "    return W, cost_history\n",
        "\n",
        "# RMSE function\n",
        "def rmse(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    Calculates the Root Mean Squared Error (RMSE).\n",
        "    \"\"\"\n",
        "    return np.sqrt(np.mean((Y - Y_pred) ** 2))\n",
        "\n",
        "# R2 function\n",
        "def r2(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    Calculates the R Squared (R²) score.\n",
        "    \"\"\"\n",
        "    mean_y = np.mean(Y)\n",
        "    ss_tot = np.sum((Y - mean_y) ** 2)\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
        "    return 1 - (ss_res / ss_tot)\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    # Step 1: Load the dataset\n",
        "    data = pd.read_csv('/content/student.csv')  # Make sure the CSV file path is correct\n",
        "\n",
        "    # Step 2: Split the data into features (X) and target (Y)\n",
        "    X = data[['Math', 'Reading']].values  # Features: Math and Reading marks\n",
        "    Y = data['Writing'].values  # Target: Writing marks\n",
        "\n",
        "    # Step 3: Split the data into training and test sets (80% train, 20% test)\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Step 4: Initialize weights (W) to zeros, learning rate, and number of iterations\n",
        "    W = np.zeros(X_train.shape[1])  # Initialize weights\n",
        "    alpha = 0.00001  # Learning rate\n",
        "    iterations = 1000  # Number of iterations for gradient descent\n",
        "\n",
        "    # Step 5: Perform Gradient Descent\n",
        "    W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
        "\n",
        "    # Step 6: Make predictions on the test set\n",
        "    Y_pred = np.dot(X_test, W_optimal)\n",
        "\n",
        "    # Step 7: Evaluate the model using RMSE and R-Squared\n",
        "    model_rmse = rmse(Y_test, Y_pred)\n",
        "    model_r2 = r2(Y_test, Y_pred)\n",
        "\n",
        "    # Step 8: Output the results\n",
        "    print(\"Final Weights:\", W_optimal)\n",
        "    print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
        "    print(\"RMSE on Test Set:\", model_rmse)\n",
        "    print(\"R-Squared on Test Set:\", model_r2)\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSNijPloeaNy",
        "outputId": "df119fcf-c4f5-433a-b846-7f5e5f0d6565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [0.34811659 0.64614558]\n",
            "Cost History (First 10 iterations): [2471.69875, 2013.165570783755, 1640.286832599692, 1337.0619994901588, 1090.4794892850578, 889.9583270083234, 726.8940993009545, 594.2897260808594, 486.4552052951635, 398.7634463599484]\n",
            "RMSE on Test Set: 5.2798239764188635\n",
            "R-Squared on Test Set: 0.8886354462786421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Present your finding:\n",
        "1. Did your Model Overfitt, Underfitts, or performance is acceptable."
      ],
      "metadata": {
        "id": "riv5LgYMe1Pa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: The model's performance appears to be acceptable. The RMSE on the test set is 5.28, which is a reasonable deviation from the actual values. The R² score is 0.8886, indicating that the model explains around 89% of the variance in the target variable, which suggests a good fit. There is no significant difference between the training and test performance, which means the model is generalizing well to unseen data."
      ],
      "metadata": {
        "id": "SamY4iYYe3Mu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Experiment with different value of learning rate, making it higher and lower, observe the result."
      ],
      "metadata": {
        "id": "cGzhRL3Pe9VT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: After experimenting with different learning rates, I found that a learning rate of 0.00001 worked well, as it allowed the cost function to decrease steadily, and the model achieved an R² of 0.8886, indicating good performance. A higher learning rate (e.g., 0.01) caused fluctuations in the cost function, suggesting that the model was not converging properly, while a lower learning rate (e.g., 0.000001) resulted in a very slow convergence, requiring more iterations to achieve similar results. Therefore, the learning rate of 0.00001 provided the best balance between convergence speed and model performance"
      ],
      "metadata": {
        "id": "qp45RBHqfBGb"
      }
    }
  ]
}